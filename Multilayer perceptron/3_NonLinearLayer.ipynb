{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Non-linear single layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jusqu'à présent, notre réseau est purement linéaire et seule une relation linéaire peut-être trouvée entre les entrées et les sorties. Ajouter d'autres couches après la première ne changerait rien à cela car on ne ferait que des opérations entre matrice; ce qui reste linéaire. Il faut donc directement ajouter une fonction non-linéaire à notre couche. Pour cela, on applique une fonction non-linéaire après avoir appliqué nos poids et notre biais aux entrées du neurone. Dans notre cas, on aimerait garder des sorties qui sont définies sur l'ensemble des réelles positifs, on utilisera donc la fonction $\\mathrm{LReLu}$ (Leaky Rectified Linear Unit) définie comme suit\n",
    "\n",
    "So far, our network is purely linear and only a linear relationship can be found between inputs and outputs. Adding other layers after the first one wouldn't change anything because we'd only be doing operations between matrices, which is still linear. So we need to add a non-linear function directly to our layer. To do this, we apply a non-linear function after applying our weights and bias to the neuron's inputs. The general consensus is that it does not matter which non-linear function is used so the simpler the better. In our case, we'll use the function $\\mathrm{LReLu}$ (Leaky Rectified Linear Unit) defined as follows\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathrm{LReLu}(x) = \\begin{cases} x, & si \\,\\, x > 0 \\\\ 0.01x, & si \\,\\, x < 0 \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "whose derivative is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\mathrm{LReLu}(x)}{\\partial x} = \\begin{cases} 1, & si \\,\\, x > 0 \\\\ 0.01, & si \\,\\, x < 0 \\end{cases}\n",
    "\\end{equation}\n",
    "which allows relatively fast numerical calculations. The output of a neuron before application of the non-linear function will be noted as\n",
    "\n",
    "\\begin{equation}\n",
    "    z_i = (\\mathbf{W} \\mathbf{x} + \\mathbf{b})_i = W_{i1} x_1 + W_{i2} x_2 + \\cdots + W_{iN_0} x_{N_0} + b_i\n",
    "\\end{equation}\n",
    "\n",
    "Each prediction is then\n",
    "\n",
    "\\begin{equation}\n",
    "    y_i = \\mathrm{LReLu}[z_i] = \\mathrm{LReLu}(W_{i1} x_1 + W_{i2} x_2 + \\cdots + W_{iN_0} x_{N_0} + b_i)\n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the errors with respect to their related weights is\n",
    "\n",
    "\\begin{aligned}\n",
    "        \\frac{\\partial \\epsilon_i}{\\partial W_{ij}} &= \\frac{\\partial}{\\partial W_{ij}} \\left(\\mathrm{LReLu}[W_{i1} x_1 + W_{i2} x_2 + \\cdots + W_{iN_0} x_{N_0} + b_i] - \\xi_i\\right)^2 \\\\\n",
    "        &= \\begin{cases} 2\\left( W_{i1} x_1 + W_{i2} x_2 + \\cdots + W_{iN_0} x_{N_0} + b_i - \\xi_i\\right)x_j, & si \\,\\, z_i > 0 \\\\ 0.02\\left( W_{i1} x_1 + W_{i2} x_2 + \\cdots + W_{iN_0} x_{N_0} + b_i - \\xi_i\\right)x_j, & si \\,\\, z_i < 0 \\end{cases} \\\\\n",
    "        & \\propto \\begin{cases} x_j \\delta_i, & si \\,\\, z_i > 0 \\\\ 0.01x_j \\delta_i, & si \\,\\, z_i < 0 \\end{cases}\n",
    "\\end{aligned}\n",
    "\n",
    "And the weights and biases are updated by the following formulae\n",
    "\n",
    "\\begin{equation}\n",
    "    W_{ij}' = \\begin{cases} W_{ij} - \\alpha x_j \\delta_i, & si \\,\\, z_i > 0 \\\\ W_{ij} - 0.01 \\alpha x_j \\delta_i, & si \\,\\, z_i < 0 \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    b_i' = \\begin{cases} b_i - \\alpha \\delta_i, & si \\,\\, z_i > 0 \\\\ b_i - 0.01 \\alpha \\delta_i, & si \\,\\, z_i < 0 \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the LRelu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LReLu (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function LReLu(p::Float64)::Float64\n",
    "    if p > 0 \n",
    "        return p\n",
    "    else\n",
    "        return 0.01*p\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define the structure of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct NonLinearOneLayerModel\n",
    "    W::Array{Float64,2}\n",
    "    b::Vector{Float64}\n",
    "    σ::Function\n",
    "end\n",
    "\n",
    "(m::NonLinearOneLayerModel)(x::Array{Float64,1}) = m.σ.(m.W * x + m.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate our model and directly test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7655795569363301, 1.1562712185784825, 2.899367462918296]\n",
      "[-0.08740704606510707, 0.6341811892530409, 1.9067158917902653]\n",
      "[0.00763999170182775, 0.4021857808024013, 3.6355654920055467]\n",
      "4.045391264509775\n"
     ]
    }
   ],
   "source": [
    "nInput = 5;\n",
    "nOutput = 3;\n",
    "\n",
    "model = NonLinearOneLayerModel(rand(nOutput,nInput), rand(nOutput), LReLu);\n",
    "\n",
    "x = rand(nInput)\n",
    "ξ = rand(nOutput)\n",
    "\n",
    "z = model.W * x + model.b\n",
    "y = model(x)\n",
    "δ = y-ξ\n",
    "ϵ = δ.^2\n",
    "ϵTot = sum(ϵ)\n",
    "println(y)\n",
    "println(δ)\n",
    "println(ϵ)\n",
    "println(ϵTot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the function to train ou neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train!(model, x, ξ, iteration, α)\n",
    "    sumϵVec = Vector{Float64}(undef,0)\n",
    "    for χ in 1:iteration\n",
    "        z = model.W * x + model.b\n",
    "        y = model(x) #Compute the prediction from the model\n",
    "        δ = y - ξ #Difference between the prediction and the expectation\n",
    "\n",
    "        #Error computed with the mean squared\n",
    "        ϵ = δ.^2\n",
    "        push!(sumϵVec, sum(ϵ))\n",
    "\n",
    "        #Update the weights\n",
    "        for j in 1:length(x)\n",
    "            for i in 1: length(ξ)\n",
    "                if z[i] > 0\n",
    "                    model.W[i,j] = model.W[i,j] -(α*x[j]*δ[i])\n",
    "                    model.b[i] = model.b[i] - α*δ[i]\n",
    "                else\n",
    "                    model.W[i,j] = model.W[i,j] -(0.01*α*x[j]*δ[i])\n",
    "                    model.b[i] = model.b[i] - 0.01*α*δ[i]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        @show sum(ϵ)\n",
    "    end\n",
    "    return sumϵVec\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(ϵ) = 4.045391264509775\n",
      "sum(ϵ) = 0.39764824819946165\n",
      "sum(ϵ) = 0.03908747484658005\n",
      "sum(ϵ) = 0.0038421662783628377\n",
      "sum(ϵ) = 0.00037767192095501385\n",
      "sum(ϵ) = 3.7123869594374905e-5\n",
      "sum(ϵ) = 3.6491505383153174e-6\n",
      "sum(ϵ) = 3.5869912799491655e-7\n",
      "sum(ϵ) = 3.5258908360628806e-8\n",
      "sum(ϵ) = 3.4658311708725877e-9\n",
      "sum(ϵ) = 3.4067945559301106e-10\n",
      "sum(ϵ) = 3.3487635643852545e-11\n",
      "sum(ϵ) = 3.2917210680830958e-12\n",
      "sum(ϵ) = 3.2356502234968344e-13\n",
      "sum(ϵ) = 3.1805344883816444e-14\n",
      "sum(ϵ) = 3.1263575729437787e-15\n",
      "sum(ϵ) = 3.0731034747223623e-16\n",
      "sum(ϵ) = 3.0207563417923686e-17\n",
      "sum(ϵ) = 2.969301603303995e-18\n",
      "sum(ϵ) = 2.918728720448692e-19\n",
      "sum(ϵ) = 2.8689899270042054e-20\n",
      "sum(ϵ) = 2.8201765917621368e-21\n",
      "sum(ϵ) = 2.7720131894426154e-22\n",
      "sum(ϵ) = 2.7253095135228936e-23\n",
      "sum(ϵ) = 2.678619235173052e-24\n",
      "sum(ϵ) = 2.6277038104192754e-25\n",
      "sum(ϵ) = 2.5804392092829585e-26\n",
      "sum(ϵ) = 2.5320586164847867e-27\n",
      "sum(ϵ) = 2.5103033118329885e-28\n",
      "sum(ϵ) = 2.354256764018957e-29\n"
     ]
    }
   ],
   "source": [
    "sumϵVec = train!(model, x, ξ, 30 , 0.1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
