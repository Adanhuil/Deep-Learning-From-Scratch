{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Single Layer Linear Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network can have several layers of neurons, this is called the depth of the network. The number of layers is noted $D$ and for this section we take $D=1$. The first thing we can generalize about a single-layer network is the number of inputs $N_0$. As there are $N_0$ inputs, there is $N_0$ weights. To simplify the notation of these inputs and weights, we can put them in the form of vectors\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{W}^\\top = \\left( \\begin{array}{ccc}\n",
    "        W_1 \\\\\n",
    "        W_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        W_{N_0} \\end{array} \\right)\\qquad \\qquad \\qquad\n",
    "     \\mathbf{x} = \\left( \\begin{array}{ccc}\n",
    "        x_1 \\\\\n",
    "        x_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_{N_0} \\end{array} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "The output of the single neuron is\n",
    "\n",
    "\\begin{equation}\n",
    "    y = \\mathbf{W} \\mathbf{x} = W_1 x_1 + W_2 x_2 + ... + W_{N_0} x_{N_0}.\n",
    "\\end{equation}\n",
    "\n",
    "The number of neurons on the last layer determines the number of outputs. In this case, there is only one layer with one neuron and therefore only one output. To increase the number of outputs, we increase the number of neurons in our layer. We denote this number $N_1$ and refer to it as the width of the layer. The $N_0$ inputs are thus each linked to $N_1$ neurons and our different weights will be organised in the form of a matrix.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{W} = \\left( \\begin{array}{ccc}\n",
    "        W_{11} & W_{12} & \\cdots & W_{1 N_0} \\\\\n",
    "        W_{21} & W_{22} & \\cdots & W_{2 N_0} \\\\\n",
    "        \\vdots & \\ddots & \\ddots & \\vdots\\\\\n",
    "        W_{N_1 1} & W_{N_1 2} & \\cdots & W_{N_1 N_0}\\end{array} \\right)\\qquad \\qquad \\qquad\n",
    "     \\mathbf{x} = \\left( \\begin{array}{ccc}\n",
    "        x_1 \\\\\n",
    "        x_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_{N_0} \\end{array} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the $i$-th output of the network is\n",
    "\n",
    "\\begin{equation}\n",
    "    y_i = (\\mathbf{W} \\mathbf{x})_i = W_{i1} x_1 + W_{i2} x_2 + \\cdots + W_{iN_0} x_{N_0}.\n",
    "\\end{equation}\n",
    "\n",
    "To go further, we can also add a bias to each prediction, which we organise in the form of a vector $\\mathbf{b}^\\top = (b_1, b_2, \\cdots, b_{N_1})$. The $i$-th prediction is therefore\n",
    "\n",
    "\\begin{equation}\n",
    "    y_i = (\\mathbf{W} \\mathbf{x} + \\mathbf{b})_i = W_{i1} x_1 + W_{2i} x_2 + \\cdots + W_{iN_0} x_{N_0} + b_i.\n",
    "\\end{equation}\n",
    "\n",
    "To give a few examples, when we want a network that takes a set of data and outputs a value, we'll simply take $N_1$ = 1. When we have an object (an image, a wave function, etc.) that we want to classify into different categories, then $N_1$ will be the number of categories and we will then apply a normalisation function at the end of the network to ascertain the probabilities that an object belongs to a specific category.\n",
    "\n",
    "To each prediction $y_i$, we can associate a difference between it and the expected value $\\xi_i$ and compute the squared error\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_i = y_i - \\xi_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\epsilon_i = (y_i - \\xi_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "the total error being simply the sum of the errors\n",
    "\n",
    "\\begin{equation}\n",
    "    \\epsilon = \\sum_{i=1}^{N_1}(y_i - \\xi_i)^2.\n",
    "\\end{equation}\n",
    "\n",
    "To train the network, we proceed in the same way as for the single neuron case. The derivative of the error associated with a weight is used to update the weight. This gives us\n",
    "\n",
    "\\begin{aligned}\n",
    "        \\frac{\\partial \\epsilon_i}{\\partial W_{ij}} &= \\frac{\\partial}{\\partial W_{ij}} \\left( W_{i1} x_1 + W_{i2} x_2 + \\cdots + W_{iN_0} x_{N_0} + b_i - \\xi_i\\right)^2 \\\\\n",
    "        &= 2\\left( W_{i1} x_1 + W_{i2} x_2 + \\cdots + W_{iN_0} x_{N_0} + b_i - \\xi_i\\right)x_j \\\\\n",
    "        &\\propto x_j \\delta_i\n",
    "\\end{aligned}\n",
    "\n",
    "And the updated weight is\n",
    "\n",
    "\\begin{equation}\n",
    "    W_{ij}' = W_{ij} - \\alpha x_j \\delta_i.\n",
    "\\end{equation}\n",
    "\n",
    "By performing the same derivative for the bias, we can see that it is updated using the following formula\n",
    "\n",
    "\\begin{equation}\n",
    "    b_i' = b_i - \\alpha \\delta_i.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we define the shape of our network using a structure and use it to define the prediction function based on the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "mutable struct LinearOneLayerModel\n",
    "    W::Array{Float64,2}\n",
    "    b::Vector{Float64}\n",
    "end\n",
    "\n",
    "(m::LinearOneLayerModel)(x) = m.W * x + m.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the number of inputs and outputs and then instantiate the model with random weights. The transposed matrix of the weights is taken directly so that the transposition does not have to be performed each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "nInput = 3;\n",
    "nOutput = 7;\n",
    "\n",
    "model = LinearOneLayerModel(rand(nOutput,nInput), rand(nOutput));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access an element of the structure, use the following syntax. We can see that we have an Array and a Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1520034834835423 0.957429029919127 0.14813348601169585; 0.9197675283814533 0.5142917560408474 0.8350692804473666; 0.1674918425016062 0.6899595716525422 0.4666664118719377; 0.5233003333324129 0.5771253393237994 0.30390153657312047; 0.8612531911678054 0.7217587533906575 0.9737972753431126; 0.9602265210095754 0.3767848423998237 0.6478058661522801; 0.9355628852936813 0.3447602646319241 0.6959130467314106]\n",
      "[0.9205607543935563, 0.33942461922785727, 0.1371652149165482, 0.6763682352642969, 0.8557160285505618, 0.7719340533560743, 0.6734687936050778]\n"
     ]
    }
   ],
   "source": [
    "println(model.W)\n",
    "println(model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then give ourselves data and test the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0298739149490298, -0.06687510423982779, -0.8132134133770582, -0.30759466427290494, 0.04739860818298969, 0.26581121440893485, 0.4391862227989465]\n",
      "[0.0008924507943818662, 0.004472279567087832, 0.6613160556963662, 0.0946144774891611, 0.002246628057684577, 0.07065560170555274, 0.19288453829640587]\n",
      "1.0270820316066402\n"
     ]
    }
   ],
   "source": [
    "x = rand(nInput)\n",
    "ξ = rand(nOutput)\n",
    "\n",
    "y = model(x)\n",
    "δ = y - ξ\n",
    "ϵ = δ.^2\n",
    "ϵTot = sum(ϵ)\n",
    "println(δ)\n",
    "println(ϵ)\n",
    "println(ϵTot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the result, we define a function that will train the network. For the record, when going through a matrix in Julia, it's quicker to do it column by column (see [performance tips](https://docs.julialang.org/en/v1/manual/performance-tips/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train!(model, x, ξ, iteration, α)\n",
    "    for χ in 1:iteration\n",
    "        y = model(x) # Compute the prediction from the model\n",
    "        δ = y - ξ # Difference between the prediction and the expectation\n",
    "\n",
    "        # Error computed with the mean squared\n",
    "        ϵ = δ.^2\n",
    "\n",
    "        # Update the weights\n",
    "        for j in 1:length(x)\n",
    "            for i in 1: length(ξ)\n",
    "                model.W[i,j] = model.W[i,j] -(α*x[j]*δ[i])\n",
    "                model.b[i] = model.b[i] - α*δ[i]\n",
    "            end\n",
    "        end\n",
    "        @show sum(ϵ)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(ϵ) = 1.0270820316066402\n",
      "sum(ϵ) = 0.333075885536838\n",
      "sum(ϵ) = 0.10801429887017776\n",
      "sum(ϵ) = 0.03502832017277846\n",
      "sum(ϵ) = 0.0113594517296399\n",
      "sum(ϵ) = 0.003683794796939677\n",
      "sum(ϵ) = 0.0011946302012579622\n",
      "sum(ϵ) = 0.00038741064484461757\n",
      "sum(ϵ) = 0.00012563470066375088\n",
      "sum(ϵ) = 4.0742499518053464e-5\n",
      "sum(ϵ) = 1.3212522163134891e-5\n",
      "sum(ϵ) = 4.284733238664879e-6\n",
      "sum(ϵ) = 1.3895105491478179e-6\n",
      "sum(ϵ) = 4.506090481363713e-7\n",
      "sum(ϵ) = 1.4612952336847586e-7\n",
      "sum(ϵ) = 4.7388834485689395e-8\n",
      "sum(ϵ) = 1.536788447775668e-8\n",
      "sum(ϵ) = 4.983702930948641e-9\n",
      "sum(ϵ) = 1.6161817808985615e-9\n",
      "sum(ϵ) = 5.241170240537911e-10\n",
      "sum(ϵ) = 1.6996767204709463e-10\n",
      "sum(ϵ) = 5.5119387874142966e-11\n",
      "sum(ϵ) = 1.7874851629236467e-11\n",
      "sum(ϵ) = 5.796695738463627e-12\n",
      "sum(ϵ) = 1.8798299520860837e-12\n",
      "sum(ϵ) = 6.096163756809324e-13\n",
      "sum(ϵ) = 1.9769454450427233e-13\n",
      "sum(ϵ) = 6.411102854417033e-14\n",
      "sum(ϵ) = 2.0790781017661814e-14\n",
      "sum(ϵ) = 6.742312273590084e-15\n"
     ]
    }
   ],
   "source": [
    "train!(model, x, ξ, 30, 0.1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
