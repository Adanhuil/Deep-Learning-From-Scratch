{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Multilayer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Dense multilayer neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a deep learning neural network, all you need to do is add layers. The number of layers in the network, denoted $D$, is called the depth of the network. Each layer is given an index $k$ and contains a weight matrix $W^{(k)}$ and a bias vector $b^{(k)}$. The $k$-th layer will have as input values the output values of the previous layer. We will only consider dense networks, i.e. neural network where each neuron of a layer is connected to all the output values of the previous layer. The weight matrix and bias vectors will therefore be of the form\n",
    "\n",
    "\\begin{equation}\n",
    "    W^{(k)} = \\left( \\begin{array}{ccc}\n",
    "        W_{11} & W_{12} & \\cdots & W_{1 N_{k-1}} \\\\\n",
    "        W_{21} & W_{22} & \\cdots & W_{2 N_1} \\\\\n",
    "        \\vdots & \\ddots & \\ddots & \\vdots\\\\\n",
    "        W_{N_k 1} & W_{N_0 2} & \\cdots & W_{N_k N_{k-1}}\\end{array} \\right)\\qquad \\qquad \\qquad\n",
    "     \\mathbf{b}^{(k)} = \\left( \\begin{array}{ccc}\n",
    "        b_1 \\\\\n",
    "        b_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        b_{N_k} \\end{array} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Before and after applying the non-linear function $\\sigma^{(k)}$ associated with the $k$-th layer, the $i$-th output of the $k$-th layer will be respectively\n",
    "\n",
    "\\begin{equation}\n",
    "    z_i^{(k)} = \\left(W^{(k)} \\mathbf{y}^{(k-1)} + \\mathbf{b}^{(k)}\\right)_i = W_{i1}^{(k)} y_1^{(k-1)} + W_{i2}^{(k)} y_2^{(k-1)} + \\cdots + W_{iN_0}^{(k)} y_{N_0}^{(k-1)} + b_i^{(k)}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    y_i^{(k)} = \\sigma^{(k)} \\left[\\left(W^{(k)} \\mathbf{y}^{(k-1)} + \\mathbf{b}^{(k)}\\right)_i \\right] = \\sigma^{(k)} \\left(W_{i1}^{(k)} y_1^{(k-1)} + W_{i2}^{(k)} y_2^{(k-1)} + \\cdots + W_{iN_0}^{(k)} y_{N_0}^{(k-1)} + b_i^{(k)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $y_{j}^{(k-1)}$ is the output of the previous layer, with $\\mathbf{y}^{(0)} = \\mathbf{x}$. At the very end of the network, the $N_D$ output values are given by\n",
    "\n",
    "\\begin{equation}\n",
    "    y_i^{(D)} = \\sigma^{(D)} \\left(W_{i1}^{(D)} y_1^{(D-1)} + W_{i2}^{(D)} y_2^{(D-1)} + \\cdots + W_{iN_0}^{(D)} y_{N_0}^{(D-1)} + b_i^{(D)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "and we can then calculate the difference between the network output and the expected output and define an error\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_i = y_i^{(D)} - \\xi_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\epsilon_i = \\left(y_i^{(D)} - \\xi_i\\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "The total error being the sum of the individual errors\n",
    "\n",
    "\\begin{equation}\n",
    "    \\epsilon = \\sum_{i=1}^{N_1}\\left(y_i^{(D)} - \\xi_i\\right)^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the non-linear function that we will give to our different layers. This function doesn't necessarily have to be the same in each layer, although that's what we'll do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLu (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function ReLu(p::Float64)::Float64\n",
    "    if p > 0\n",
    "        return p\n",
    "    else\n",
    "        return 0.0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a single layer as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct SingleLayer\n",
    "    W::Array{Float64,2}\n",
    "    b::Vector{Float64}\n",
    "    σ::Function\n",
    "end\n",
    "\n",
    "(l::SingleLayer)(x::Array{Float64,1}) = l.σ.(l.W * x + l.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multilayer network is made up of a set of single layers. We therefore store the single layers in a vector in the multilayer structure. Next, we create a function on the multilayer structure that will pass a vector of input data through the entire network. This is based on the function defined on the single layer structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct MultiLayerModel\n",
    "    Layers::Vector{SingleLayer}\n",
    "end\n",
    "\n",
    "function (m::MultiLayerModel)(x::Array{Float64,1})\n",
    "    y = x\n",
    "    \n",
    "    for i in 1:length(m.Layers)\n",
    "        y = m.Layers[i](y)\n",
    "    end\n",
    "    \n",
    "    return y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to simplify the creation of a multilayer network, we define two functions: Dense and Chain. The Dense function creates a single layer based on the number of inputs and outputs in that layer and its non-linear function. If no non-linear function is specified, the identity function will simply be applied. The Chain function takes a certain number of simple layers as arguments and generates the multi-layer network corresponding to the order of the layers given as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function Dense(nInput::Int64,nOutput::Int64, σ::Function=identity)\n",
    "    return SingleLayer(rand(nOutput,nInput), rand(nOutput), σ)\n",
    "end\n",
    "\n",
    "function Chain(SingleLayers::SingleLayer...)\n",
    "    Layers = Vector{SingleLayer}(undef,length(SingleLayers))\n",
    "    for k in 1:length(SingleLayers)\n",
    "        Layers[k] = SingleLayers[k]\n",
    "    end\n",
    "    return MultiLayerModel(Layers)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now instantiate the multilayer network using the Chain and Dense functions. We can now pass an input vector through the network to obtain its final outputs and the associated error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49.195426458436515, 47.90973784360255, 28.49225628130916]\n",
      "[48.19634544937155, 47.284638318825074, 27.610533821848247]\n",
      "[2322.8877146751574, 2235.8370209421005, 762.341577927426]\n",
      "5321.066313544683\n"
     ]
    }
   ],
   "source": [
    "n0 = 5;\n",
    "n1 = 8;\n",
    "n2 = 7;\n",
    "n3 = 3;\n",
    "\n",
    "#Créé un réseau à trois couches avec respectivement n1, n2 et n3 neurones. La fonction non-linéaire est ReLu\n",
    "#sur les deux premières couches et on ne met rien sur la troisième.\n",
    "model = Chain(Dense(n0,n1, ReLu),Dense(n1,n2, ReLu),Dense(n2,n3))\n",
    "\n",
    "y0 = rand(n0)\n",
    "ξ = rand(n3)\n",
    "\n",
    "yD = model(y0)\n",
    "δ = yD-ξ\n",
    "ϵ = δ.^2\n",
    "ϵTot = sum(ϵ)\n",
    "println(yD)\n",
    "println(δ)\n",
    "println(ϵ)\n",
    "println(ϵTot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Back-propagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An historical problem with multilayer networks has been to find a way of training them correctly and efficiently (numerically speaking). The $\\epsilon$ error explicitly contains only the weights of the last layer and not those of the previous layers. We can therefore use the method found for the single layer to update the weights of the $D$ layer via\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left(W_{ij}^{(D)}\\right)' = W_{ij}^{(D)} - \\alpha y_j^{(D-1)} \\delta_i.\n",
    "\\end{equation}\n",
    "\n",
    "However, this method cannot be used to update the weights of previous layers. In fact, what is the $\\delta_i$ difference that would allow the weights to be pushed in the right direction? We therefore need to find a correctly defined $\\delta^{(k)}$ for each $k$-th layer.\n",
    "\n",
    "To do this, let's start as always with the derivative of the error as a function of a certain weight in a certain layer\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\epsilon}{\\partial W_{ij}^{(k)}} = 2 \\sum_{l=1}^{N_D} \\left( y_l^{(D)} - \\xi_l \\right)\\frac{\\partial y_l^{(D)}}{\\partial W_{ij}^{(k)}}.\n",
    "\\end{equation}\n",
    "\n",
    "Using the derivation theorem for compound functions, we obtain\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial y_l^{(D)}}{\\partial W_{ij}^{(k)}} = \\left( \\sigma^{(D)} \\right)' \\left( z_l^{(D)}\\right)\\frac{\\partial z_l^{(D)}}{\\partial W_{ij}^{(k)}}\n",
    "\\end{equation}\n",
    "\n",
    "where $z_l^{(D)} = \\sum_{m=1}^{N_{D-1}} W_{lm}^{(D)} y_m^{(D-1)}+b_m^{(D)}$. If $k=D$, then we obtain the usual formula with $y_j^{(D-1)}\\delta_i$. On the other hand, if $k<D$, we need to push the derivation further.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial z_l^{(D)}}{\\partial W_{ij}^{(k)}} = \\sum_{m=1}^{N_{D-1}} W_{lm}^{(D)} \\frac{\\partial y_m^{(D-1)}}{\\partial W_{ij}^{(k)}}.\n",
    "\\end{equation}\n",
    "\n",
    "Here we can see a pattern taking shape as, starting from the derivative $\\frac{\\partial y_l^{(D)}}{\\partial W_{ij}^{(k)}}$, we arrive at the derivative in the previous layer $\\frac{\\partial y_m^{(D-1)}}{\\partial W_{ij}^{(k)}}$. To make this pattern clear, for any layer $k'>k$ we can write\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial z_l^{(k')}}{\\partial W_{ij}^{(k)}} = \\sum_{m=1}^{N_{k'-1}} W_{lm}^{(k')} \\left( \\sigma^{(k'-1)} \\right)' \\left( z_m^{(k'-1)}\\right) \\frac{\\partial z_m^{(k'-1)}}{\\partial W_{ij}^{(k)}}.\n",
    "\\end{equation}\n",
    "\n",
    "If we define at each layer $k'>1$ the matrix $\\mathbf{M}^{(k’)}$ whose elements are\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{M}^{(k')}_{lm} = W_{lm}^{(k')}\\left( \\sigma^{(k'-1)} \\right)' \\left( z_m^{(k'-1)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "we can see that the above equation can be written as a matrix product\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial z_l^{(k')}}{\\partial W_{ij}^{(k)}} = \\left[ \\mathbf{M}^{(k')} \\frac{\\partial \\mathbf{z}^{(k'-1)}}{\\partial W_{ij}^{(k)}} \\right]_l.\n",
    "\\end{equation}\n",
    "\n",
    "Finally, we obtain the formula for the back-propagation algorithm using this matrix product of $\\mathbf{M}^{(k')}$ on each layer\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial z_l^{(D)}}{\\partial W_{ij}^{(k)}} = \\left[ \\mathbf{M}^{(D)}\\mathbf{M}^{(D-1)} \\cdots \\mathbf{M}^{(k+2)}\\mathbf{M}^{(k+1)} \\frac{\\partial \\mathbf{z}^{(k)}}{\\partial W_{ij}^{(k)}}\\right]_l.\n",
    "\\end{equation}\n",
    "\n",
    "In this way, we arrive at the correct layer for calculating the derivative of $\\mathbf{z}$ as a function of its explicit weights\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial z_l^{(k)}}{\\partial W_{ij}^{(k)}} = y_j^{(k-1)} \\delta_{il}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The back-propagation algorithm takes the following form\n",
    "\n",
    "1. Forward Propagation : Pass the input $y^{(0)}$ through the neural network. For each layer, calculate the associated $\\mathbf{M}$ matrix.\n",
    "\n",
    "2. Once the end of the network has been reached, calculate the initial deviation vector\n",
    "\n",
    "    \\begin{equation}\n",
    "        \\Delta_l^{(D)} = \\left(  y_l^{(D)} - \\xi_l \\right) \\left( \\sigma^{(D)}\\right)'\\left( z_l^{(D)}\\right)\n",
    "    \\end{equation}\n",
    "    \n",
    "3. Backward Propagation : For $k$ from $D$ to $1$, update the weights using the formula\n",
    "\n",
    "    \\begin{equation}\n",
    "        \\left(W_{ij}^{(k)}\\right)' = W_{ij}^{(k)} - \\alpha\\Delta_i^{(k)}y_j^{(k-1)}\n",
    "    \\end{equation}\n",
    "\n",
    "    If $k>1$, update the deviation vector\n",
    "\n",
    "    \\begin{equation}\n",
    "        \\Delta_l^{(k-1)} = \\sum_{l'=1}^{N_{k}} \\Delta_{l'}^{(k)} M_{l'l}^{(k)}\n",
    "    \\end{equation}\n",
    "    \n",
    "Once the algorithm had been completed, all the weights in all the layers were updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function for training the multilayer network defined previously on the basis of the back-propagation algorithm. Be careful with the indices, as yMat[1] corresponds to $y^{(0)}$ and M[k] corresponds to $\\mathbf{M}^{(k+1)}$. Since the last layer has no non-linear function, $\\Delta^{(D)}=\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train!(model::MultiLayerModel, x::Array{Float64,1}, ξ::Array{Float64,1}, iteration::Int64, α::Float64)\n",
    "    sumϵVec = Vector{Float64}(undef,0)\n",
    "    D = length(model.Layers)\n",
    "    zMat = Vector{Vector{Float64}}(undef, D)\n",
    "    yMat = Vector{Vector{Float64}}(undef, D+1)\n",
    "    yMat[1] = x\n",
    "    M = Vector{Array{Float64,2}}(undef, D-1)\n",
    "\n",
    "    for χ in 1:iteration\n",
    "        \n",
    "        #Forward propagation\n",
    "        for k in 1:D\n",
    "            \n",
    "            zMat[k] = model.Layers[k].W * yMat[k] + model.Layers[k].b\n",
    "            yMat[k+1] = model.Layers[k].σ.(zMat[k])\n",
    "\n",
    "            if k < D\n",
    "                Mk = Array{Float64,2}(undef, size(model.Layers[k+1].W)[1], size(model.Layers[k+1].W)[2])\n",
    "                for j in 1:size(model.Layers[k+1].W)[2]\n",
    "                    for i in 1:size(model.Layers[k+1].W)[1]\n",
    "                        if zMat[k][j] > 0\n",
    "                            Mk[i,j] = model.Layers[k+1].W[i,j]\n",
    "                        else\n",
    "                            Mk[i,j] = 0\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "                M[k] = Mk\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        Δ = transpose(yMat[D+1]-ξ)\n",
    "        ϵ = Δ.^2\n",
    "        @show sum(ϵ)\n",
    "        push!(sumϵVec, sum(ϵ))\n",
    "\n",
    "        #Backward Propagation\n",
    "        for k in D:-1:1\n",
    "            for j in 1:size(model.Layers[k].W)[2]\n",
    "                for i in 1:size(model.Layers[k].W)[1]\n",
    "                    model.Layers[k].W[i,j] -= α*Δ[i]*yMat[k][j]\n",
    "                    model.Layers[k].b[i] -= α*Δ[i]\n",
    "                end\n",
    "            end\n",
    "            if k > 1\n",
    "                Δ = Δ*M[k-1]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return sumϵVec\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(ϵ) = 5321.066313544683\n",
      "sum(ϵ) = 2603.037824258862\n",
      "sum(ϵ) = 234.27340418329754\n",
      "sum(ϵ) = 21.084606376496794\n",
      "sum(ϵ) = 1.8976145738847108\n",
      "sum(ϵ) = 0.1707853116496239\n",
      "sum(ϵ) = 0.015370678048466167\n",
      "sum(ϵ) = 0.0013833610243619468\n",
      "sum(ϵ) = 0.00012450249219258128\n",
      "sum(ϵ) = 1.1205224297333643e-5\n",
      "sum(ϵ) = 1.0084701867603453e-6\n",
      "sum(ϵ) = 9.076231680833754e-8\n",
      "sum(ϵ) = 8.168608512803568e-9\n",
      "sum(ϵ) = 7.35174766139525e-10\n",
      "sum(ϵ) = 6.61657289581038e-11\n",
      "sum(ϵ) = 5.954915607047867e-12\n",
      "sum(ϵ) = 5.359424045009763e-13\n",
      "sum(ϵ) = 4.8234816269188204e-14\n",
      "sum(ϵ) = 4.3411334710598604e-15\n",
      "sum(ϵ) = 3.907020082314531e-16\n",
      "sum(ϵ) = 3.516318460471024e-17\n",
      "sum(ϵ) = 3.1646865205514392e-18\n",
      "sum(ϵ) = 2.848215507772521e-19\n",
      "sum(ϵ) = 2.563404389284905e-20\n",
      "sum(ϵ) = 2.3070762599048053e-21\n",
      "sum(ϵ) = 2.0763006326880429e-22\n",
      "sum(ϵ) = 1.8684003129084435e-23\n",
      "sum(ϵ) = 1.682042970567843e-24\n",
      "sum(ϵ) = 1.5152881536554053e-25\n",
      "sum(ϵ) = 1.3584887367149536e-26\n"
     ]
    }
   ],
   "source": [
    "sumϵVec = train!(model, y0, ξ, 30 , 0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you now know how a neural network works ! 🥳"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
