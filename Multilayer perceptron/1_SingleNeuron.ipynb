{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the following, we will consider the simplest problem that can be encountered in machine learning. We have an unknown function $x\\rightarrow f(x) \\in \\mathbb{R}$ and a set of points $\\mathcal{S}=\\{f\\left(x_i\\right), x_i \\in \\mathbb{R}\\}$ sampled from our function $f$. The goal is to find an approximate function of $f$ that is satisfactory for the real-world problem at hand. We will see in the following how neural network, with the help of deep learning, can handle these problems. The learning process where the correct answer $f(x_i)$ for each input $x_i$ is known is called supervised learning, as it is similar to a teacher knowing the answer and guiding the student to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Single neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "We begin our exploration of neural networks with their fundamental unit : a simple neuron with one input and one output. To the neuron is associated a weight $W$ and it is feeded with an input $x$ coming from the samples $\\mathcal{S}$. Based on this, the neuron outputs\n",
    "\n",
    "\\begin{equation*}\n",
    "    y = Wx.\n",
    "\\end{equation*}\n",
    "\n",
    "In supervised learning, the right answer $\\xi$ is known. Hence, we can quantify the difference between $\\xi$ and the prediction $y$ of the neuron\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\delta = y-\\xi = Wx-\\xi\n",
    "\\end{equation*}\n",
    "\n",
    "A mean to quantify the error of the neuron is the squared deviation\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\epsilon = \\left(y-\\xi \\right)^2 = \\left(W x-\\xi \\right)^2\n",
    "\\end{equation*}\n",
    "\n",
    "The goal in the learning process is to minimize $\\epsilon$ with respect to the weight $W$. For this, gradient descent is used, where we take the slope of the function $W\\rightarrow\\epsilon(W)$ to push the weight $W$ in the direction of the minimum of the function. Note that in the case of the squared deviation, the slope is given by\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial \\epsilon}{\\partial W} &= \\frac{\\partial}{\\partial W} \\left( Wx - \\xi\\right)^2 \\\\\n",
    "                                        &= 2\\left( Wx - \\xi\\right)x \\\\\n",
    "                                        & \\propto \\delta x \\\\     \n",
    "\\end{align}\n",
    "\n",
    "If the slope is too steep, we might push the weight too far and go beyond the minimum. To avoid this, we define a learning rate $\\alpha$ which is a given parameter of the learning process. Finally, we update the weight with the following formula\n",
    "\n",
    "\\begin{equation}\n",
    "    W' = W-\\alpha\\delta x\n",
    "\\end{equation}\n",
    "\n",
    "Now, let's look at how to code it in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia code\n",
    "\n",
    "We define a single neuron as a new structure that encodes a weight, $W$. This weight must be updated, so the structure has to be mutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "mutable struct singleNeuronModel\n",
    "    W::Float64\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define on this structure a function which gives the prediction of the neuron as a product of the weight with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "(m::singleNeuronModel)(x) = m.W * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a single neuron with a random weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "singleNeuronModel(0.9234242035653861)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = singleNeuronModel(rand())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this neuron with some dummy data and compute the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.61712101782693\n",
      "2.61712101782693\n",
      "6.8493224219514675\n"
     ]
    }
   ],
   "source": [
    "x = 5\n",
    "ξ = 2\n",
    "δ = model(x)- ξ\n",
    "ϵ = δ^2\n",
    "println(model(x))\n",
    "println(δ)\n",
    "println(ϵ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, as the weight is random, so is the initial error. We now want to train our neuron to reduce the error. To do this, we update the weight recursively with the help of gradient descent. In the train! function, we denote the input $x\\equiv x_\\mathrm{train}$ and the right answer $\\xi\\equiv y_\\mathrm{train}$ as these are the training data on which the neuron can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train!(model, xtrain, ytrain, α, iteration)\n",
    "    for i in 1:iteration\n",
    "        y = model(xtrain) # Compute the prediction from the model\n",
    "        δ = y - ytrain # Difference between the prediction and the expectation\n",
    "        ϵ = δ^2 # Error computed with the mean squared\n",
    "        model.W = model.W -(δ*xtrain*α) # Update the weight\n",
    "        @show ϵ\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes as arguments our neuron, an input and the corresponding answer but also a learning rate $\\alpha$ and the number of times that we want the weight to be updated. Let's see how the error evolves at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ϵ = 0.00025613844039918266\n",
      "ϵ = 0.00014407787272453492\n",
      "ϵ = 8.104380340755488e-5\n",
      "ϵ = 4.558713941675112e-5\n",
      "ϵ = 2.5642765921919132e-5\n",
      "ϵ = 1.4424055831082886e-5\n",
      "ϵ = 8.113531404981593e-6\n",
      "ϵ = 4.5638614153030954e-6\n",
      "ϵ = 2.567172046107991e-6\n",
      "ϵ = 1.4440342759357448e-6\n",
      "ϵ = 8.122692802142568e-7\n",
      "ϵ = 4.569014701205194e-7\n",
      "ϵ = 2.5700707694279217e-7\n",
      "ϵ = 1.4456648078023616e-7\n",
      "ϵ = 8.131864543888285e-8\n",
      "ϵ = 4.57417380593716e-8\n",
      "ϵ = 2.5729727658396525e-8\n",
      "ϵ = 1.4472971807874759e-8\n",
      "ϵ = 8.141046641889483e-9\n",
      "ϵ = 4.579338736062834e-9\n",
      "ϵ = 2.575878039035344e-9\n",
      "ϵ = 1.4489313969658332e-9\n",
      "ϵ = 8.150239107869421e-10\n",
      "ϵ = 4.584509498176549e-10\n",
      "ϵ = 2.578786592759966e-10\n",
      "ϵ = 1.4505674584007381e-10\n",
      "ϵ = 8.159441953704723e-11\n",
      "ϵ = 4.589686099109336e-11\n",
      "ϵ = 2.581698430410536e-11\n",
      "ϵ = 1.452205367190543e-11\n"
     ]
    }
   ],
   "source": [
    "train!(model, x, ξ, 0.01, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error reduces as expected ! Of course, a single neuron is unable to approximate complex functions. To get more flexibility, we need more neurons and some sort of non-linearity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
